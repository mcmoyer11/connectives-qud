---
title: "Pre-processing Data from Pilot 2: Connectives + QUD Study - Specific"
author: Morgan Moyer
date: January 10, 2023
output: html_document
---

```{r setup, include=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)
library(lme4)
library(lmerTest)
library(multcomp) # not available for this version of R
library(stringr)
library(textstem)
library(tidyverse)
theme_set(theme_bw())
cbPalette <- c("#56B4E9", "#D55E00", "#009E73","#999999", "#E69F00")
this.dir <- dirname(rstudioapi::getSourceEditorContext()$path)
setwd(this.dir)
source("../../helpers.R")
```

# THis one from website
```{r}
# User-defined function to read in PCIbex Farm results files
read.pcibex <- function(filepath, auto.colnames=TRUE, fun.col=function(col,cols){cols[cols==col]<-paste(col,"Ibex",sep=".");return(cols)}) {
  n.cols <- max(count.fields(filepath,sep=",",quote=NULL),na.rm=TRUE)
  if (auto.colnames){
    cols <- c()
    con <- file(filepath, "r")
    while ( TRUE ) {
      line <- readLines(con, n = 1, warn=FALSE)
      if ( length(line) == 0) {
        break
      }
      m <- regmatches(line,regexec("^# (\\d+)\\. (.+)\\.$",line))[[1]]
      if (length(m) == 3) {
        index <- as.numeric(m[2])
        value <- m[3]
        if (is.function(fun.col)){
         cols <- fun.col(value,cols)
        }
        cols[index] <- value
        if (index == n.cols){
          break
        }
      }
    }
    close(con)
    return(read.csv(filepath, comment.char="#", header=FALSE, col.names=cols))
  }
  else{
    return(read.csv(filepath, comment.char="#", header=FALSE, col.names=seq(1:n.cols)))
  }
}

```


```{r}
# REad in the results

d_tot <- read.pcibex("../data/results.csv")

d_tot$ID = as.factor(d_tot$ID)
```


```{r}
# View(d)
nrow(d_tot) #4555
nrow(d_tot) / 20 # 395.8

length(unique(d_tot$ID)) # 39

# names(d_tot)
```

# Take a look at comments and Problems
```{r}
unique(d_tot$PennElementType)
comments <- d_tot %>% filter((PennElementType == "TextInput") & (PennElementName == "Comments"))
unique(comments$Value)

comments <- d_tot %>% filter((PennElementType == "TextInput") & (PennElementName == "Problems"))
unique(comments$Value)

comments <- d_tot %>% filter((PennElementType == "TextInput") & (PennElementName == "NativeLang"))
unique(comments$Value)

comments <- d_tot %>% filter((PennElementType == "TextInput") & (PennElementName == "OtherLangs"))

unique(comments$Value)

comments <- d_tot %>% filter((PennElementType == "TextInput") & (PennElementName == "Gender"))
unique(comments$Value)
```


# Just look at fillers and critical
```{r}
d <- d_tot %>% 
  filter(Label %in% c("fillers","critical"))
```



# Fillers

## TRUE RESPONSE FILLERS
```{r}
facc_pos <- d %>% filter(Number == 3 |
                           Number == 4 |
                           Number == 7 |
                           Number == 8 |
                           Number == 11 |
                           Number == 12)
# For these numbers, Pressed_key should be true / F
facc_pos$Accuracy = ifelse(facc_pos$Value[facc_pos$Parameter=="PressedKey"] == "F",1,0)
```
- near ceiling performance for 'true' response fillers
```{r, graph fillers}
agg <- facc_pos %>%
  select(ID, Number, Accuracy) %>%
  group_by(ID) %>%
  mutate(mean_accuracy = mean(Accuracy))

dodge = position_dodge(.9)
ggplot(data=agg, aes(x=reorder(ID,mean_accuracy),y=mean_accuracy,fill=ID)) +
  geom_bar(position=dodge,stat="identity")

```


```{r}
agg <- facc_pos %>%
  select(Number, Accuracy) %>%
  group_by(Number) %>%
  mutate(mean_accuracy = mean(Accuracy))

dodge = position_dodge(.9)
ggplot(data=agg, aes(x=reorder(Number,mean_accuracy),y=mean_accuracy,fill=Number)) +
  geom_bar(position=dodge,stat="identity")


```

## FALSE RESPONSE FILLERS
```{r}
facc_neg <- d %>% filter(Number == 1 |
                           Number == 2 |
                           Number == 5 |
                           Number == 6 |
                           Number == 9 |
                           Number == 10)
# For numbers 19-21 (Number >= 19) correct Pressed_key should be false / J
facc_neg$Accuracy = ifelse(facc_neg$Value[facc_neg$Parameter=="PressedKey"] == "J",1,0)
```

- really low accuracy on 'false' response fillers
```{r, graph false response fillers}
agg <- facc_neg %>%
  select(Number, Accuracy) %>%
  group_by(Number) %>%
  mutate(mean_accuracy = mean(Accuracy))

dodge = position_dodge(.9)
ggplot(data=agg, aes(x=reorder(Number,mean_accuracy),y=mean_accuracy,fill=Number)) +
  geom_bar(position=dodge,stat="identity")


# hmm this is problematic...there seems to be a clear yes-bias here....
```

```{r, graph fillers}
agg <- facc_neg %>%
  select(ID, Number, Accuracy) %>%
  group_by(ID) %>%
  mutate(mean_accuracy = mean(Accuracy))

dodge = position_dodge(.9)
ggplot(data=agg, aes(x=reorder(ID,mean_accuracy),y=mean_accuracy,fill=ID)) +
  geom_bar(position=dodge,stat="identity")

```

### Look total fillers by-subject mean accuracy
```{r}
facc = rbind(facc_neg,facc_pos)
nrow(facc_pos)
# View(facc)
agg <- facc %>%
  select(ID, Number, Accuracy) %>%
  group_by(ID) %>%
  summarize(mean_accuracy = mean(Accuracy))
# View(agg)

dodge = position_dodge(.9)
ggplot(data=agg, aes(x=reorder(ID,mean_accuracy),y=mean_accuracy,fill=ID)) +
  geom_bar(position=dodge,stat="identity")
```

### by-item
```{r, graph false response fillers}
agg <- facc %>%
  select(Number, Accuracy) %>%
  group_by(Number) %>%
  mutate(mean_accuracy = mean(Accuracy))

dodge = position_dodge(.9)
ggplot(data=agg, aes(x=reorder(Number,mean_accuracy),y=mean_accuracy,fill=Number)) +
  geom_bar(position=dodge,stat="identity")
```

```{r, eval=FALSE}
# Remove subjects whose accuracy falls below 75%\ >>> not finished

summary(facc$Accuracy)
inac <- facc %>% 
  subset(Accuracy > .75)

nrow(facc) - nrow(inac)
length(unique(facc$ID))
length(unique(inac$ID))
```


# Look at overall trial time
```{r}
# View(d)
d$TrialType <- as.factor(d$TrialType)
levels(d$TrialType)

d$SentType <- as.factor(d$SentType)
levels(d$SentType)

d_trialTime = d %>%
    filter(Label %in% c("fillers","critical") & Parameter == "_Trial_" ) %>%
    select(-c("Results.reception.time","Controller.name","PennElementType","PennElementName")) %>%
    group_by(ID.Ibex,Letters) %>%
    summarise( trialRT = EventTime[Value=="End"] - EventTime[Value=="Start"]) 


summary(d_trialTime$trialRT)
```

```{r}
ggplot(d_trialTime, aes(x=trialRT)) +
  geom_density(alpha = .4)
  # geom_histogram(stat="count")

```


```{r}
# Concat ID and Word columns together to the 
d_trialTime$unique <- paste(d_trialTime$ID.Ibex,d_trialTime$Letters,sep="_")

# View(d_trialTime)

d_wordTime <- d %>%
  # Filter to the important lines of the file
  filter(PennElementName == "DashedSentence" | PennElementName == "select") %>%
  select(ID.Ibex,Letters,TrialType,Number,SentType,QUDTruth,QUDType,QUD,AnswerConj1,AnswerConj2,Conj,PennElementName,Reading.time,Value) 

# View(d_wordTime)

# create a new column that differentiates between the two "dashedSentences" so 
# as "chunk1" and "chunk2" so we can spread wide
rename <- c("chunk1","chunk2","select")
dWT <- cbind(rename,d_wordTime) 
# creat a column with the unique combination of ID and item so that we can join
# them together in a couple steps
dWT$unique <- paste(d_wordTime$ID.Ibex,d_wordTime$Letters,sep="_")

# spread "rename" column with reading time values
dWT_wide <- dWT %>%
    group_by(ID.Ibex,Letters,TrialType,Number,SentType,QUDTruth,QUDType,QUD,AnswerConj1,AnswerConj2,Conj,PennElementName,Reading.time,Value) %>%
    pivot_wider(names_from=rename,values_from=Reading.time)

# View(dWT_wide)
names(d_trialTime)

# joing the dfs together
df <- left_join(dWT_wide,d_trialTime)
names(df)
```



# Remove subjects with RT higher than 3x IQR
```{r, trialRT outliers}

summary(df$trialRT)
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# 2359    4713    6006    6553    7862   20458 
range(df$trialRT)
# 2359 20458

hist(df$trialRT, breaks=100, col="red", xlab="RT (ms)",
        main="Histogram with Normal Curve")

# remove subjects with RT higher than 3 x IQR
cutoff <- quantile(df$trialRT,na.rm=TRUE)[4] + IQR(df$trialRT,na.rm=TRUE)*3 # 17309.75 
df.outliers.removed <- subset(df, df$trialRT < cutoff)

hist(df.outliers.removed$trialRT, col="red", xlab="RT (ms)",
        main="Histogram with Normal Curve")

summary(df.outliers.removed$trialRT)
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# 2359    4709    5998    6498    7813   15616 
```


# Add Decision RT
(overall trial RT) - chunk 2 RT

```{r, decision rt}
df.outliers.removed$DecisionRT <- as.numeric(df.outliers.removed$trialRT) - as.numeric(df.outliers.removed$chunk2)
```

## Trying to clean up DF so there aren't weird extra na values for measures of 
interest (chunk1, chunk2, decisionRT)

### First make Forced-choice response column
```{r}
df.outliers.removed <- df.outliers.removed %>% 
  group_by(unique) %>% 
  mutate(Response = Value[PennElementName == "select"]) %>% 
  # drop the columns 'Value' and 'select'
  select(-c(Value,select))

# View(df.outliers.removed)
```


```{r}
fax <- df.outliers.removed[,c("PennElementName","unique","Response","chunk1","chunk2","DecisionRT")]

View(df.outliers.removed)
# figure out how many lines in fax
n_rep <- nrow(fax)/3

# make df with new names repeating as many rows as fax
new_names <- rep(c("DashedSentence1","DashedSentence2","select"),times=n_rep)
df1 <- data.frame(new_names)
# merge with fax
fax_rename <- cbind(fax,df1) %>% 
  # drop old PennElementName and rename new column
  select(-c("PennElementName"))
# rename new_names column to PennElementName
names(fax_rename)[names(fax_rename) == 'new_names'] <- 'PennElementName'

# subset to remove the NaN values
a <- na.omit(fax_rename[,c("unique","chunk1")])
b <- na.omit(fax_rename[,c("unique","chunk2")])
c <- na.omit(fax_rename[,c("unique","DecisionRT")])
# for some reason there are duplicates for these so use unique()
d <- unique(fax_rename[,c("unique","Response")])

# combine them all together
good <- Reduce(function(x,y) merge(x = x, y = y, by = "unique"), 
       list(a,b,c,d))

# reduce the remaining factors to unique values as well
cons <- df.outliers.removed %>% 
  select(-c("PennElementName","Response","chunk1","chunk2","DecisionRT")) %>% 
  unique()
  
total_good <- merge(good,cons,by="unique") 
# View(total_good)  
```


# Save to CSV for easy graphing
```{r}
write.csv(total_good,"../data/processed.csv")
```
